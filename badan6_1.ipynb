{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ù…Ø±Ø­Ù„Ù‡ ÛŒ Ø§ÙˆÙ„\n",
    "#Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø§Ù… Ø­Ø±Ú©Øª ÙˆØ±Ø²Ø´ÛŒ\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "import csv\n",
    "\n",
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ YOLO (Ù†Ø³Ø®Ù‡ Ø³Ø¨Ú©)\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# ØªÙ†Ø¸ÛŒÙ… MediaPipe Pose Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ø³Ú©Ù„Øª Ø¨Ø¯Ù†\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=1)\n",
    "\n",
    "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Optical Flow (Lucas-Kanade)\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
    "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ\n",
    "output_dir = \"keypoints_output_name_1\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def save_keypoints_to_csv(keypoints, exercise_name, frame_number):\n",
    "    \"\"\" Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ CSV Ø¨Ø§ Ù†Ø§Ù… Ø­Ø±Ú©Øª ÙˆØ±Ø²Ø´ÛŒ \"\"\"\n",
    "    output_file = os.path.join(output_dir, f\"{exercise_name}_frame_{frame_number}.csv\")\n",
    "    with open(output_file, mode='w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['x', 'y'])  # Ø¹Ù†ÙˆØ§Ù† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§\n",
    "        for pt in keypoints:\n",
    "            writer.writerow(pt)  # Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ\n",
    "\n",
    "def process_video(video_path, exercise_name):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙˆÛŒØ¯ÛŒÙˆ: {video_path}\")\n",
    "        return\n",
    "\n",
    "    # Ú©Ø§Ù‡Ø´ Ø±Ø²ÙˆÙ„ÙˆØ´Ù† Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ù‡ 320x240\n",
    "    prev_frame = cv2.resize(prev_frame, (320, 240))\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    prev_keypoints = []  # Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ Ù‚Ø¨Ù„ÛŒ (Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù„ÛŒØ³Øª Ø§Ø² (x, y))\n",
    "    frame_count = 0\n",
    "\n",
    "    yolo_interval = 10  # Ù‡Ø± 10 ÙØ±ÛŒÙ…ØŒ ØªØ´Ø®ÛŒØµ YOLO/MediaPipe Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Ú©Ø§Ù‡Ø´ Ø±Ø²ÙˆÙ„ÙˆØ´Ù† ÙØ±ÛŒÙ… Ø¨Ù‡ 320x240\n",
    "        frame = cv2.resize(frame, (320, 240))\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_count += 1\n",
    "\n",
    "        if frame_count % yolo_interval == 5:\n",
    "            # Ø§Ø¬Ø±Ø§ÛŒ YOLO Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ ÙˆØ±Ø²Ø´Ú©Ø§Ø± (ÙÙ‚Ø· Ø§Ù†Ø³Ø§Ù†)\n",
    "            results = model(frame)\n",
    "            new_keypoints = []\n",
    "            for result in results:\n",
    "                for box in result.boxes:\n",
    "                    if int(box.cls) == 0:  # Ú©Ù„Ø§Ø³ 0 Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø³Ø§Ù†\n",
    "                        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                        x1 = max(0, x1); y1 = max(0, y1)\n",
    "                        x2 = min(frame.shape[1], x2); y2 = min(frame.shape[0], y2)\n",
    "                        roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "                        # Ø§Ø¬Ø±Ø§ÛŒ MediaPipe Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù‚Ø§Ø· Ø¨Ø¯Ù† Ø§Ø² ROI\n",
    "                        rgb_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "                        results_pose = pose.process(rgb_roi)\n",
    "                        if results_pose.pose_landmarks:\n",
    "                            for lm in results_pose.pose_landmarks.landmark:\n",
    "                                x = int(x1 + lm.x * (x2 - x1))\n",
    "                                y = int(y1 + lm.y * (y2 - y1))\n",
    "                                new_keypoints.append((x, y))\n",
    "            if new_keypoints:\n",
    "                prev_keypoints = new_keypoints\n",
    "                save_keypoints_to_csv(new_keypoints, exercise_name, frame_count)\n",
    "        else:\n",
    "            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø§ Optical Flow Ø¯Ø± ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒÙ† ØªØ´Ø®ÛŒØµ\n",
    "            if prev_keypoints:\n",
    "                p0 = np.array(prev_keypoints, dtype=np.float32).reshape(-1, 1, 2)\n",
    "                p1, st, err = cv2.calcOpticalFlowPyrLK(prev_gray, gray, p0, None, **lk_params)\n",
    "                if p1 is not None:\n",
    "                    updated_points = []\n",
    "                    for i, (new, status) in enumerate(zip(p1, st)):\n",
    "                        if status[0] == 1:\n",
    "                            x, y = new.ravel()\n",
    "                            updated_points.append((int(x), int(y)))\n",
    "                    if updated_points:\n",
    "                        prev_keypoints = updated_points\n",
    "\n",
    "        # Ø±Ø³Ù… Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ Ø±ÙˆÛŒ ÙØ±ÛŒÙ…\n",
    "        for pt in prev_keypoints:\n",
    "            cv2.circle(frame, pt, 3, (0, 255, 0), -1)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        prev_gray = gray.copy()\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…Ø³ÛŒØ± ÙˆÛŒØ¯ÛŒÙˆÙ‡Ø§ Ø§Ø² Ù¾ÙˆØ´Ù‡\n",
    "video_dir = r\"D:\\aiocup\\challesh_5\\New folder (3)\\badansazi2\"\n",
    "video_paths = []\n",
    "for folder_name in os.listdir(video_dir):\n",
    "    folder_path = os.path.join(video_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if file_path.endswith(('.mp4', '.avi')):\n",
    "                exercise_name = folder_name  # Ù†Ø§Ù… Ø­Ø±Ú©Øª ÙˆØ±Ø²Ø´ÛŒ Ø§Ø² Ù¾ÙˆØ´Ù‡ Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
    "                video_paths.append((file_path, exercise_name))\n",
    "\n",
    "# Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙˆÛŒØ¯ÛŒÙˆÙ‡Ø§\n",
    "for vp, exercise in video_paths:\n",
    "    process_video(vp, exercise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For exercise 'Ø§Ø³Ú©ÙˆØ§Øª (Squats)', expected keypoints: 33\n",
      "For exercise 'Ø¬Ù„Ùˆ Ø¨Ø§Ø²Ùˆ (Bicep Curls)', expected keypoints: 33\n",
      "For exercise 'Ø¯Ø¯Ù„ÛŒÙØª (Deadlifts)', expected keypoints: 33\n",
      "For exercise 'ÙÙ„Ø§ÛŒ Ø³ÛŒÙ†Ù‡ (Chest Fly)', expected keypoints: 33\n",
      "For exercise 'Ù„Øª (Lat Pulldown)', expected keypoints: 33\n",
      "For exercise 'Ù„Ù†Ø¬ (Lunges)', expected keypoints: 33\n",
      "For exercise 'Ù¾Ø±Ø³ Ø³ÛŒÙ†Ù‡ (Bench Press)', expected keypoints: 33\n",
      "For exercise 'Ù¾Ø±Ø³ Ø´Ø§Ù†Ù‡ (Overhead Shoulder Press)', expected keypoints: 33\n",
      "For exercise 'Ù¾Ø±Ø³ Ù¾Ø§ (Leg Press)', expected keypoints: 33\n",
      "For exercise 'Ù¾Ø´Øª Ø¨Ø§Ø²Ùˆ (Tricep Dips)', expected keypoints: 33\n",
      "For exercise 'Ù¾Ù„Ø§Ù†Ú© (Plank)', expected keypoints: 33\n",
      "For exercise 'Ù¾ÙˆØ´ Ø¢Ù¾ (Push-ups)', expected keypoints: 33\n",
      "For exercise 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)', expected keypoints: 33\n",
      "For exercise 'Ú©Ø±Ø§Ø³ Ø§ÙˆØ± (Cable Crossover)', expected keypoints: 33\n",
      "For exercise 'Ú©Ø±Ø§Ù†Ú† (Crunches)', expected keypoints: 33\n",
      "File 'Ø¬Ù„Ùˆ Ø¨Ø§Ø²Ùˆ (Bicep Curls)_frame_105.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ø¬Ù„Ùˆ Ø¨Ø§Ø²Ùˆ (Bicep Curls)_frame_115.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ø¬Ù„Ùˆ Ø¨Ø§Ø²Ùˆ (Bicep Curls)_frame_135.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ø¬Ù„Ùˆ Ø¨Ø§Ø²Ùˆ (Bicep Curls)_frame_155.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ø¬Ù„Ùˆ Ø¨Ø§Ø²Ùˆ (Bicep Curls)_frame_35.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ø¬Ù„Ùˆ Ø¨Ø§Ø²Ùˆ (Bicep Curls)_frame_55.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ø¬Ù„Ùˆ Ø¨Ø§Ø²Ùˆ (Bicep Curls)_frame_65.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ø¬Ù„Ùˆ Ø¨Ø§Ø²Ùˆ (Bicep Curls)_frame_95.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ø¯Ø¯Ù„ÛŒÙØª (Deadlifts)_frame_415.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ø¯Ø¯Ù„ÛŒÙØª (Deadlifts)_frame_615.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ø¯Ø¯Ù„ÛŒÙØª (Deadlifts)_frame_885.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù„Øª (Lat Pulldown)_frame_105.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù„Øª (Lat Pulldown)_frame_115.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù„Øª (Lat Pulldown)_frame_135.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù„Øª (Lat Pulldown)_frame_145.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù„Øª (Lat Pulldown)_frame_15.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù„Øª (Lat Pulldown)_frame_155.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù„Øª (Lat Pulldown)_frame_45.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù„Øª (Lat Pulldown)_frame_55.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù„Øª (Lat Pulldown)_frame_65.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù„Øª (Lat Pulldown)_frame_865.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù„Øª (Lat Pulldown)_frame_995.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù„Ù†Ø¬ (Lunges)_frame_105.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù„Ù†Ø¬ (Lunges)_frame_115.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù„Ù†Ø¬ (Lunges)_frame_65.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù„Ù†Ø¬ (Lunges)_frame_95.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾Ø±Ø³ Ø³ÛŒÙ†Ù‡ (Bench Press)_frame_675.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾Ø±Ø³ Ø³ÛŒÙ†Ù‡ (Bench Press)_frame_685.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾Ø´Øª Ø¨Ø§Ø²Ùˆ (Tricep Dips)_frame_185.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆØ´ Ø¢Ù¾ (Push-ups)_frame_2715.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_1015.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_1105.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_1245.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_1255.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_1345.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_1355.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_1365.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_1375.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_1385.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_1395.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_1625.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_1645.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_2145.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_245.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_255.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_265.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_275.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_295.csv': cropped from 99 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_345.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_355.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_375.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_385.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_395.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_405.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_435.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_455.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_625.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_655.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_685.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_695.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_825.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_835.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_855.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_915.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_935.csv': cropped from 99 to 33 keypoints.\n",
      "File 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)_frame_975.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1045.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_105.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1055.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1125.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1135.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1145.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_115.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1205.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1215.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1225.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1295.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1305.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_135.csv': cropped from 99 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1355.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1365.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1435.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1445.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_145.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_15.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1515.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1525.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1535.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_155.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1605.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1615.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_165.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1685.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_1695.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_175.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_35.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_435.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_445.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_45.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_515.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_525.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_55.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_575.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_585.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_65.csv': cropped from 99 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_655.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_665.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_735.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_745.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_755.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_825.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_835.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_85.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_905.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_915.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_95.csv': cropped from 66 to 33 keypoints.\n",
      "File 'Ú©Ø±Ø§Ù†Ú† (Crunches)_frame_965.csv': cropped from 66 to 33 keypoints.\n",
      "âœ… Ø§ØµÙ„Ø§Ø­ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!\n"
     ]
    }
   ],
   "source": [
    "#Ù…Ø±Ø­Ù„Ù‡ Ø¯ÙˆÙ…\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡ Ø­Ø§ÙˆÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ CSV Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡\n",
    "csv_dir = \"keypoints_output_name_1\"\n",
    "\n",
    "# Ù¾ÙˆØ´Ù‡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„Ø§Ø­â€ŒØ´Ø¯Ù‡\n",
    "fixed_dir = \"fixed_keypoints_output\"\n",
    "os.makedirs(fixed_dir, exist_ok=True)\n",
    "\n",
    "# Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ø­Ø±Ú©Øª ÙˆØ±Ø²Ø´ÛŒ\n",
    "exercise_files = {}\n",
    "for file in os.listdir(csv_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        exercise_name = file.split(\"_frame_\")[0]\n",
    "        exercise_files.setdefault(exercise_name, []).append(file)\n",
    "\n",
    "# ØªØ¹ÛŒÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø­Ø±Ú©Øª Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø­Ø§Ù„Øª (mode)\n",
    "expected_counts = {}\n",
    "for exercise, files in exercise_files.items():\n",
    "    counts = []\n",
    "    for f in files:\n",
    "        file_path = os.path.join(csv_dir, f)\n",
    "        df = pd.read_csv(file_path)\n",
    "        counts.append(df.shape[0])  # ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ (Ù‡Ø± Ø±Ø¯ÛŒÙ ÛŒÚ© Ù†Ù‚Ø·Ù‡ Ú©Ù„ÛŒØ¯ÛŒ)\n",
    "    # ØªØ¹ÛŒÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ø±Ø§ÛŒØ¬ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ\n",
    "    count_mode = collections.Counter(counts).most_common(1)[0][0]\n",
    "    expected_counts[exercise] = count_mode\n",
    "    print(f\"For exercise '{exercise}', expected keypoints: {count_mode}\")\n",
    "\n",
    "# Ø§ØµÙ„Ø§Ø­ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ø¬Ø¯ÛŒØ¯\n",
    "for exercise, files in exercise_files.items():\n",
    "    expected = expected_counts[exercise]\n",
    "    for f in files:\n",
    "        file_path = os.path.join(csv_dir, f)\n",
    "        df = pd.read_csv(file_path)\n",
    "        current_count = df.shape[0]\n",
    "        fixed_df = df.copy()\n",
    "        \n",
    "        if current_count > expected:\n",
    "            # Ø¨Ø±Ø´ (crop) Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ\n",
    "            fixed_df = fixed_df.iloc[:expected, :]\n",
    "            print(f\"File '{f}': cropped from {current_count} to {expected} keypoints.\")\n",
    "        elif current_count < expected:\n",
    "            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ (NaN) Ø¨Ø±Ø§ÛŒ ØªÚ©Ù…ÛŒÙ„\n",
    "            pad_count = expected - current_count\n",
    "            pad_df = pd.DataFrame(np.nan, index=range(pad_count), columns=df.columns)\n",
    "            fixed_df = pd.concat([fixed_df, pad_df], ignore_index=True)\n",
    "            print(f\"File '{f}': padded with {pad_count} missing keypoints.\")\n",
    "        \n",
    "        # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ Ø§ØµÙ„Ø§Ø­â€ŒØ´Ø¯Ù‡ Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ Ù‡Ù…Ø§Ù† Ù†Ø§Ù… ÙØ§ÛŒÙ„\n",
    "        output_path = os.path.join(fixed_dir, f)\n",
    "        fixed_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"âœ… Ø§ØµÙ„Ø§Ø­ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª 'Ø§Ø³Ú©ÙˆØ§Øª (Squats)' Ø¯Ø± ÙØ§ÛŒÙ„ 'final_datasets_by_exercise_1\\Ø§Ø³Ú©ÙˆØ§Øª (Squats).csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\n",
      "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª 'Ø¬Ù„Ùˆ Ø¨Ø§Ø²Ùˆ (Bicep Curls)' Ø¯Ø± ÙØ§ÛŒÙ„ 'final_datasets_by_exercise_1\\Ø¬Ù„Ùˆ Ø¨Ø§Ø²Ùˆ (Bicep Curls).csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\n",
      "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª 'Ø¯Ø¯Ù„ÛŒÙØª (Deadlifts)' Ø¯Ø± ÙØ§ÛŒÙ„ 'final_datasets_by_exercise_1\\Ø¯Ø¯Ù„ÛŒÙØª (Deadlifts).csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\n",
      "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª 'ÙÙ„Ø§ÛŒ Ø³ÛŒÙ†Ù‡ (Chest Fly)' Ø¯Ø± ÙØ§ÛŒÙ„ 'final_datasets_by_exercise_1\\ÙÙ„Ø§ÛŒ Ø³ÛŒÙ†Ù‡ (Chest Fly).csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\n",
      "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª 'Ù„Øª (Lat Pulldown)' Ø¯Ø± ÙØ§ÛŒÙ„ 'final_datasets_by_exercise_1\\Ù„Øª (Lat Pulldown).csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\n",
      "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª 'Ù„Ù†Ø¬ (Lunges)' Ø¯Ø± ÙØ§ÛŒÙ„ 'final_datasets_by_exercise_1\\Ù„Ù†Ø¬ (Lunges).csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\n",
      "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª 'Ù¾Ø±Ø³ Ø³ÛŒÙ†Ù‡ (Bench Press)' Ø¯Ø± ÙØ§ÛŒÙ„ 'final_datasets_by_exercise_1\\Ù¾Ø±Ø³ Ø³ÛŒÙ†Ù‡ (Bench Press).csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\n",
      "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª 'Ù¾Ø±Ø³ Ø´Ø§Ù†Ù‡ (Overhead Shoulder Press)' Ø¯Ø± ÙØ§ÛŒÙ„ 'final_datasets_by_exercise_1\\Ù¾Ø±Ø³ Ø´Ø§Ù†Ù‡ (Overhead Shoulder Press).csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\n",
      "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª 'Ù¾Ø±Ø³ Ù¾Ø§ (Leg Press)' Ø¯Ø± ÙØ§ÛŒÙ„ 'final_datasets_by_exercise_1\\Ù¾Ø±Ø³ Ù¾Ø§ (Leg Press).csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\n",
      "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª 'Ù¾Ø´Øª Ø¨Ø§Ø²Ùˆ (Tricep Dips)' Ø¯Ø± ÙØ§ÛŒÙ„ 'final_datasets_by_exercise_1\\Ù¾Ø´Øª Ø¨Ø§Ø²Ùˆ (Tricep Dips).csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\n",
      "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª 'Ù¾Ù„Ø§Ù†Ú© (Plank)' Ø¯Ø± ÙØ§ÛŒÙ„ 'final_datasets_by_exercise_1\\Ù¾Ù„Ø§Ù†Ú© (Plank).csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\n",
      "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª 'Ù¾ÙˆØ´ Ø¢Ù¾ (Push-ups)' Ø¯Ø± ÙØ§ÛŒÙ„ 'final_datasets_by_exercise_1\\Ù¾ÙˆØ´ Ø¢Ù¾ (Push-ups).csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\n",
      "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª 'Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups)' Ø¯Ø± ÙØ§ÛŒÙ„ 'final_datasets_by_exercise_1\\Ù¾ÙˆÙ„ Ø¢Ù¾ (Pull-ups).csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\n",
      "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª 'Ú©Ø±Ø§Ø³ Ø§ÙˆØ± (Cable Crossover)' Ø¯Ø± ÙØ§ÛŒÙ„ 'final_datasets_by_exercise_1\\Ú©Ø±Ø§Ø³ Ø§ÙˆØ± (Cable Crossover).csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\n",
      "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª 'Ú©Ø±Ø§Ù†Ú† (Crunches)' Ø¯Ø± ÙØ§ÛŒÙ„ 'final_datasets_by_exercise_1\\Ú©Ø±Ø§Ù†Ú† (Crunches).csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\n"
     ]
    }
   ],
   "source": [
    "#Ù…Ø±Ø­Ù„Ù‡ Ø³ÙˆÙ… Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡ Ø­Ø§ÙˆÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ CSV\n",
    "csv_dir = \"fixed_keypoints_output\"\n",
    "\n",
    "# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù¾ÙˆØ´Ù‡\n",
    "if not os.path.exists(csv_dir):\n",
    "    raise FileNotFoundError(f\"âŒ Ù…Ø³ÛŒØ± '{csv_dir}' ÛŒØ§ÙØª Ù†Ø´Ø¯!\")\n",
    "\n",
    "# Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ø­Ø±Ú©Øª ÙˆØ±Ø²Ø´ÛŒ\n",
    "# Ú©Ù„ÛŒØ¯: Ù†Ø§Ù… Ø­Ø±Ú©Øª ÙˆØ±Ø²Ø´ÛŒØŒ Ù…Ù‚Ø¯Ø§Ø±: Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡\n",
    "dataset_dict = {}\n",
    "\n",
    "# Ù„ÛŒØ³Øª ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ CSV Ø¯Ø± Ù¾ÙˆØ´Ù‡\n",
    "csv_files = [f for f in os.listdir(csv_dir) if f.endswith(\".csv\")]\n",
    "if not csv_files:\n",
    "    raise ValueError(\"âŒ Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ CSVâ€ŒØ§ÛŒ Ø¯Ø± Ù…Ø³ÛŒØ± Ù…Ø´Ø®Øµâ€ŒØ´Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯!\")\n",
    "\n",
    "# Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø­Ø±Ú©Øª (Ø¨Ø±Ø§ÛŒ ÛŒÚ©Ù¾Ø§Ø±Ú†Ú¯ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§)\n",
    "expected_num_keypoints = {}\n",
    "\n",
    "# Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ CSV\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(csv_dir, file)\n",
    "    \n",
    "    try:\n",
    "        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ø­Ø±Ú©Øª ÙˆØ±Ø²Ø´ÛŒ Ø§Ø² Ù†Ø§Ù… ÙØ§ÛŒÙ„ (Ù‚Ø¨Ù„ Ø§Ø² \"_frame_\")\n",
    "        exercise_name = file.split(\"_frame_\")[0]\n",
    "\n",
    "        # Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø®ØªØ§Ø± ØµØ­ÛŒØ­ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "        if df.shape[1] != 2 or list(df.columns) != [\"x\", \"y\"]:\n",
    "            print(f\"âš ï¸ ÙØ§ÛŒÙ„ '{file}' Ø¯Ø§Ø±Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯!\")\n",
    "            continue\n",
    "        \n",
    "        # ØªØ¨Ø¯ÛŒÙ„ Ù†Ù‚Ø§Ø· (x, y) Ø¨Ù‡ ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø± ÙˆÛŒÚ˜Ú¯ÛŒ (Ø¢Ø±Ø§ÛŒÙ‡ ÛŒÚ©â€ŒØ¨Ø¹Ø¯ÛŒ)\n",
    "        keypoints = df.to_numpy().flatten()\n",
    "        \n",
    "        # Ø¨Ø±Ø±Ø³ÛŒ ÛŒÚ©Ù¾Ø§Ø±Ú†Ú¯ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø­Ø±Ú©Øª\n",
    "        if exercise_name not in expected_num_keypoints:\n",
    "            expected_num_keypoints[exercise_name] = len(keypoints)\n",
    "        elif len(keypoints) != expected_num_keypoints[exercise_name]:\n",
    "            print(f\"âš ï¸ ÙØ§ÛŒÙ„ '{file}' Ø¯Ø§Ø±Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ Ù…ØªÙØ§ÙˆØª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª '{exercise_name}' Ø§Ø³Øª Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯!\")\n",
    "            continue\n",
    "        \n",
    "        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ø±Ø¯Ø§Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ù‡ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ\n",
    "        if exercise_name not in dataset_dict:\n",
    "            dataset_dict[exercise_name] = []\n",
    "        dataset_dict[exercise_name].append(keypoints)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ '{file}': {e}\")\n",
    "\n",
    "# Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ù¾Ø³ Ø§Ø² ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù†ØŒ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© Ø¯Ø§Ø¯Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯\n",
    "if not dataset_dict:\n",
    "    raise ValueError(\"âŒ Ù¾Ø³ Ø§Ø² ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ØŒ Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø§Ù‚ÛŒ Ù†Ù…Ø§Ù†Ø¯!\")\n",
    "\n",
    "# Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ø­Ø±Ú©Øª ÙˆØ±Ø²Ø´ÛŒ\n",
    "output_dir = \"final_datasets_by_exercise_1\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù‡Ø± Ø¯ÛŒØªØ§Ø³Øª Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ø­Ø±Ú©Øª ÙˆØ±Ø²Ø´ÛŒ\n",
    "for exercise, features_list in dataset_dict.items():\n",
    "    if not features_list:\n",
    "        continue\n",
    "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ Ùˆ ØªÙ‚Ø³ÛŒÙ… Ø¢Ù† Ø¨Ù‡ x Ùˆ y\n",
    "    num_keypoints = expected_num_keypoints[exercise]\n",
    "    num_points = num_keypoints // 2\n",
    "    \n",
    "    # ØªØ¹Ø±ÛŒÙ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…\n",
    "    col_names = [f\"x{i}\" for i in range(num_points)] + [f\"y{i}\" for i in range(num_points)]\n",
    "    \n",
    "    # Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª Ø¬Ø§Ø±ÛŒ\n",
    "    df_ex = pd.DataFrame(features_list, columns=col_names)\n",
    "    \n",
    "    # Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª Ø¬Ø§Ø±ÛŒ\n",
    "    output_file = os.path.join(output_dir, f\"{exercise}.csv\")\n",
    "    \n",
    "    # Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¯Ø± ÙØ§ÛŒÙ„ CSV\n",
    "    df_ex.to_csv(output_file, index=False)\n",
    "    print(f\"âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª '{exercise}' Ø¯Ø± ÙØ§ÛŒÙ„ '{output_file}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ø´Ú©Ù„ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (X): (1261, 66)\n",
      "âœ… Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ÛŒÚ©ØªØ§: ['Ø§Ø³Ú©ÙˆØ§Øª (Squats)' 'Ø¬Ù„Ùˆ Ø¨Ø§Ø²Ùˆ (Bicep Curls)' 'Ø¯Ø¯Ù„ÛŒÙØª (Deadlifts)' 'ÙÙ„Ø§ÛŒ Ø³ÛŒÙ†Ù‡ (Chest Fly)' 'Ù„Øª (Lat Pulldown)' 'Ù¾Ø±Ø³ Ø³ÛŒÙ†Ù‡ (Bench Press)' 'Ù¾Ø±Ø³ Ù¾Ø§ (Leg Press)' 'Ù¾ÙˆØ´ Ø¢Ù¾ (Push-ups)' 'Ú©Ø±Ø§Ø³ Ø§ÙˆØ± (Cable Crossover)' 'Ú©Ø±Ø§Ù†Ú† (Crunches)']\n",
      "âœ… Scaler Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: lstm_scaler.pkl\n",
      "âœ… LabelEncoder Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: label_encoder.pkl\n",
      "âœ… ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡: Ø¢Ù…ÙˆØ²Ø´ = 1008 Ù†Ù…ÙˆÙ†Ù‡, ØªØ³Øª = 253 Ù†Ù…ÙˆÙ†Ù‡\n",
      "âœ… Ø´Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´: (1008, 66, 1)\n",
      "âœ… Ø´Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª: (253, 66, 1)\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_5 (Conv1D)           (None, 64, 64)            256       \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 32, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 32, 64)            0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,090\n",
      "Trainable params: 38,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "32/32 [==============================] - 5s 44ms/step - loss: 2.1408 - accuracy: 0.2510 - val_loss: 1.9965 - val_accuracy: 0.3004\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 2.0321 - accuracy: 0.2956 - val_loss: 1.9502 - val_accuracy: 0.3004\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.9007 - accuracy: 0.3383 - val_loss: 1.6329 - val_accuracy: 0.4625\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.4252 - accuracy: 0.5367 - val_loss: 1.0768 - val_accuracy: 0.6838\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.1198 - accuracy: 0.6260 - val_loss: 0.9015 - val_accuracy: 0.6838\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.0059 - accuracy: 0.6597 - val_loss: 0.8085 - val_accuracy: 0.7154\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9267 - accuracy: 0.6786 - val_loss: 0.7611 - val_accuracy: 0.7233\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.8591 - accuracy: 0.7093 - val_loss: 0.7306 - val_accuracy: 0.7036\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.8392 - accuracy: 0.7093 - val_loss: 0.6654 - val_accuracy: 0.7431\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.7584 - accuracy: 0.7411 - val_loss: 0.6341 - val_accuracy: 0.7945\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.7172 - accuracy: 0.7569 - val_loss: 0.5861 - val_accuracy: 0.8103\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.6885 - accuracy: 0.7619 - val_loss: 0.5982 - val_accuracy: 0.7866\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.6586 - accuracy: 0.7659 - val_loss: 0.6178 - val_accuracy: 0.7589\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.6546 - accuracy: 0.7659 - val_loss: 0.5280 - val_accuracy: 0.8063\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.6904 - accuracy: 0.7758 - val_loss: 0.5400 - val_accuracy: 0.8221\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.6061 - accuracy: 0.7937 - val_loss: 0.4738 - val_accuracy: 0.8577\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.5701 - accuracy: 0.8274 - val_loss: 0.4721 - val_accuracy: 0.8221\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.5718 - accuracy: 0.8125 - val_loss: 0.4323 - val_accuracy: 0.8617\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.5195 - accuracy: 0.8185 - val_loss: 0.3639 - val_accuracy: 0.8696\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.5023 - accuracy: 0.8343 - val_loss: 0.3786 - val_accuracy: 0.8775\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.4807 - accuracy: 0.8651 - val_loss: 0.3482 - val_accuracy: 0.8854\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.4279 - accuracy: 0.8710 - val_loss: 0.3028 - val_accuracy: 0.9051\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.4703 - accuracy: 0.8433 - val_loss: 0.4182 - val_accuracy: 0.8577\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.3999 - accuracy: 0.8750 - val_loss: 0.2656 - val_accuracy: 0.9368\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.3780 - accuracy: 0.8929 - val_loss: 0.2825 - val_accuracy: 0.9328\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.3766 - accuracy: 0.8770 - val_loss: 0.3462 - val_accuracy: 0.8933\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.3634 - accuracy: 0.8919 - val_loss: 0.2041 - val_accuracy: 0.9407\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.4439 - accuracy: 0.8542 - val_loss: 0.4814 - val_accuracy: 0.8221\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.3760 - accuracy: 0.8889 - val_loss: 0.2639 - val_accuracy: 0.9170\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.3371 - accuracy: 0.8958 - val_loss: 0.2647 - val_accuracy: 0.9249\n",
      "âœ… Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: lstm_model.h5\n"
     ]
    }
   ],
   "source": [
    "#Ù…Ø±Ø­Ù„Ù‡ Ú†Ù‡Ø§Ø±Ù…\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import joblib\n",
    "\n",
    "# ----------------------------- #\n",
    "# Ù…Ø±Ø­Ù„Ù‡ Û±: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§\n",
    "# ----------------------------- #\n",
    "\n",
    "# Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ø­Ø±Ú©Øª ÙˆØ±Ø²Ø´ÛŒ\n",
    "data_dir = \"final_datasets_by_exercise_1\"\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "# Ù‡Ø± ÙØ§ÛŒÙ„ CSV ÛŒÚ© Ø­Ø±Ú©Øª ÙˆØ±Ø²Ø´ÛŒ Ù…Ø´Ø®Øµ Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        # Ù†Ø§Ù… Ø­Ø±Ú©Øª ÙˆØ±Ø²Ø´ÛŒ Ø§Ø² Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ø¨Ø¯ÙˆÙ† Ù¾Ø³ÙˆÙ†Ø¯)\n",
    "        label = file.replace(\".csv\", \"\")\n",
    "        df = pd.read_csv(os.path.join(data_dir, file))\n",
    "        # ÙØ±Ø¶ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ù‡Ø± Ø±Ø¯ÛŒÙ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ (ÛŒÚ© ÙØ±ÛŒÙ…) Ø§Ø² Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø³Øª\n",
    "        X_data.extend(df.values)\n",
    "        y_data.extend([label] * len(df))\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "print(\"âœ… Ø´Ú©Ù„ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (X):\", X_data.shape)\n",
    "print(\"âœ… Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ÛŒÚ©ØªØ§:\", np.unique(y_data))\n",
    "\n",
    "# ----------------------------- #\n",
    "# Ù…Ø±Ø­Ù„Ù‡ Û²: Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "# ----------------------------- #\n",
    "\n",
    "# Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Ø°Ø®ÛŒØ±Ù‡ scaler Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ\n",
    "joblib.dump(scaler, \"lstm_scaler.pkl\")\n",
    "print(\"âœ… Scaler Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: lstm_scaler.pkl\")\n",
    "\n",
    "# Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_data)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# Ø°Ø®ÛŒØ±Ù‡ label encoder\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "print(\"âœ… LabelEncoder Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: label_encoder.pkl\")\n",
    "\n",
    "# ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ùˆ ØªØ³Øª\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_categorical, test_size=0.2, random_state=42, stratify=y_categorical\n",
    ")\n",
    "print(f\"âœ… ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡: Ø¢Ù…ÙˆØ²Ø´ = {X_train.shape[0]} Ù†Ù…ÙˆÙ†Ù‡, ØªØ³Øª = {X_test.shape[0]} Ù†Ù…ÙˆÙ†Ù‡\")\n",
    "\n",
    "# ØªØºÛŒÛŒØ± Ø´Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ CNN-LSTM:\n",
    "# ÙØ±Ø¶: Ù‡Ø± Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© ÙØ±ÛŒÙ… Ø§Ø³ØªØ› Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† ØªÙˆØ§Ù„ÛŒ Ø·ÙˆÙ„ 1 Ø¯Ø§Ø±ÛŒÙ…\n",
    "# Ø§Ú¯Ø± Ø¯Ø± Ø¢ÛŒÙ†Ø¯Ù‡ ØªÙˆØ§Ù„ÛŒâ€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯ ÙØ±ÛŒÙ…ÛŒ Ø¯Ø§Ø±ÛŒØ¯ØŒ Ø¨Ø§ÛŒØ¯ Ø§ÛŒÙ† Ù‚Ø³Ù…Øª ØªØºÛŒÛŒØ± Ú©Ù†Ø¯.\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test  = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "print(\"âœ… Ø´Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´:\", X_train.shape)\n",
    "print(\"âœ… Ø´Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª:\", X_test.shape)\n",
    "\n",
    "# ----------------------------- #\n",
    "# Ù…Ø±Ø­Ù„Ù‡ Û³: Ø³Ø§Ø®Øª Ù…Ø¯Ù„ CNN-LSTM\n",
    "# ----------------------------- #\n",
    "\n",
    "num_features = X_train.shape[1]  # ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ (Ù…Ø«Ù„Ø§Ù‹ 66 Ø§Ú¯Ø± 33 Ù†Ù‚Ø·Ù‡ Ø¨Ø§ Û² Ù…Ø®ØªØµØ§Øª)\n",
    "num_classes = y_categorical.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "# Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ CNN Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­Ù„ÛŒ\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(num_features, 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "# Ù„Ø§ÛŒÙ‡ LSTM Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ (Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø·ÙˆÙ„ ØªÙˆØ§Ù„ÛŒ Ø¨Ø±Ø§Ø¨Ø± 1 Ø§Ø³ØªØ› Ø¨Ø±Ø§ÛŒ ØªÙˆØ§Ù„ÛŒâ€ŒÙ‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ± Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø¯Ø§Ø±Ø¯)\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# ----------------------------- #\n",
    "# Ù…Ø±Ø­Ù„Ù‡ Û´: Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n",
    "# ----------------------------- #\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# ----------------------------- #\n",
    "# Ù…Ø±Ø­Ù„Ù‡ Ûµ: Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡\n",
    "# ----------------------------- #\n",
    "\n",
    "model.save(\"lstm_model.h5\")\n",
    "print(\"âœ… Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: lstm_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Keypoints shape: (66,)\n",
      "ğŸ“Œ Keypoints sample: [    0.32086     0.67493     0.31178     0.66364     0.31195     0.66177     0.31217     0.65976     0.31168     0.66354]\n",
      "1/1 [==============================] - 1s 606ms/step\n",
      "Ù†ÙˆØ¹ Ø­Ø±Ú©Øª ÙˆØ±Ø²Ø´ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡: Ù¾ÙˆØ´ Ø¢Ù¾ (Push-ups)\n"
     ]
    }
   ],
   "source": [
    "#Ù…Ø±Ø­Ù„Ù‡ Ù¾Ù†Ø¬Ù…\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ scaler Ùˆ label encoder\n",
    "MODEL_PATH = \"lstm_model.h5\"\n",
    "SCALER_PATH = \"lstm_scaler.pkl\"\n",
    "LABEL_ENCODER_PATH = \"label_encoder.pkl\"\n",
    "\n",
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ØŒ scaler Ùˆ label encoder\n",
    "model = load_model(MODEL_PATH)\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "label_encoder = joblib.load(LABEL_ENCODER_PATH)\n",
    "\n",
    "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=1)\n",
    "# ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ: ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ 33 Ù†Ù‚Ø·Ù‡ Ø¨Ø§ 2 Ù…Ø®ØªØµØ§Øª (x,y) Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
    "NUM_FEATURES = 33 * 2\n",
    "\n",
    "def extract_keypoints(frame):\n",
    "    \"\"\"\n",
    "    Ø§Ø² ÛŒÚ© ÙØ±ÛŒÙ…ØŒ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ (x,y) Ø¨Ø¯Ù† Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
    "    Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… Ø´Ù†Ø§Ø³Ø§ÛŒÛŒØŒ ÛŒÚ© Ø¢Ø±Ø§ÛŒÙ‡ Ø§Ø² ØµÙØ±Ù‡Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.\n",
    "    \"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "    keypoints = []\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            keypoints.append(landmark.x)\n",
    "            keypoints.append(landmark.y)\n",
    "    else:\n",
    "        keypoints = [0.0] * NUM_FEATURES\n",
    "    return np.array(keypoints)\n",
    "\n",
    "def predict_exercise_from_video(video_path):\n",
    "    # Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† ÙˆÛŒØ¯ÛŒÙˆ\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† ÙˆÛŒØ¯ÛŒÙˆ: {video_path}\")\n",
    "        return None\n",
    "\n",
    "    # ØªØ¹ÛŒÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ ÙØ±ÛŒÙ… Ù…ÛŒØ§Ù†ÛŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù†Ù…ÙˆÙ†Ù‡ Ù†Ù…Ø§ÛŒÙ†Ø¯Ù‡\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    mid_frame_no = frame_count // 2\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame_no)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ±ÛŒÙ… Ø§Ø² ÙˆÛŒØ¯ÛŒÙˆ.\")\n",
    "        return None\n",
    "\n",
    "    # ØªØºÛŒÛŒØ± Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ±ÛŒÙ… (Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²ØŒ Ù‡Ù…Ø§Ù† Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ØŒ Ù…Ø«Ù„Ø§Ù‹ 320x240)\n",
    "    frame = cv2.resize(frame, (320, 240))\n",
    "\n",
    "    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ\n",
    "    keypoints = extract_keypoints(frame)\n",
    "    if keypoints.shape[0] != NUM_FEATURES:\n",
    "        print(\"âŒ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ Ù†Ø§Ø¯Ø±Ø³Øª Ø§Ø³Øª.\")\n",
    "        return None\n",
    "\n",
    "    # ØªØºÛŒÛŒØ± Ø´Ú©Ù„ Ø¨Ù‡ ØµÙˆØ±Øª (1, ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§, 1) Ú†ÙˆÙ† Ù…Ø¯Ù„ Ù…Ø§ Ø§Ù†ØªØ¸Ø§Ø± ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø§ Ø´Ú©Ù„ (batch, features, 1) Ø¯Ø§Ø±Ø¯\n",
    "    input_data = keypoints.reshape(1, NUM_FEATURES)\n",
    "    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ scaler Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡\n",
    "    input_scaled = scaler.transform(input_data)\n",
    "    # ØªØºÛŒÛŒØ± Ø´Ú©Ù„ Ø¨Ù‡ Ø³Ù‡ Ø¨Ø¹Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ (1, NUM_FEATURES, 1)\n",
    "    input_final = input_scaled.reshape(1, NUM_FEATURES, 1)\n",
    "\n",
    "    print(\"ğŸ” Keypoints shape:\", keypoints.shape)\n",
    "    print(\"ğŸ“Œ Keypoints sample:\", keypoints[:10])  # Ø¯Ù‡ ØªØ§ Ø§ÙˆÙ„ÛŒ Ø±Ùˆ Ú†Ø§Ù¾ Ú©Ù†\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ù…Ø¯Ù„\n",
    "    prediction_probs = model.predict(input_final)\n",
    "    predicted_index = np.argmax(prediction_probs, axis=1)[0]\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_index])[0]\n",
    "    return predicted_label\n",
    "\n",
    "# Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡:\n",
    "video_file = \"ppp.mp4\"  # Ù…Ø³ÛŒØ± ÙˆÛŒØ¯ÛŒÙˆÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯\n",
    "predicted_exercise = predict_exercise_from_video(video_file)\n",
    "if predicted_exercise is not None:\n",
    "    print(\"Ù†ÙˆØ¹ Ø­Ø±Ú©Øª ÙˆØ±Ø²Ø´ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡:\", predicted_exercise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØ´Ù†Ø§Ø³Ù‡: ['Ø§Ø³Ú©ÙˆØ§Øª (Squats)' 'Ø¬Ù„Ùˆ Ø¨Ø§Ø²Ùˆ (Bicep Curls)' 'Ø¯Ø¯Ù„ÛŒÙØª (Deadlifts)' 'ÙÙ„Ø§ÛŒ Ø³ÛŒÙ†Ù‡ (Chest Fly)' 'Ù„Øª (Lat Pulldown)' 'Ù¾Ø±Ø³ Ø³ÛŒÙ†Ù‡ (Bench Press)' 'Ù¾Ø±Ø³ Ù¾Ø§ (Leg Press)' 'Ù¾ÙˆØ´ Ø¢Ù¾ (Push-ups)' 'Ú©Ø±Ø§Ø³ Ø§ÙˆØ± (Cable Crossover)' 'Ú©Ø±Ø§Ù†Ú† (Crunches)']\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØ´Ù†Ø§Ø³Ù‡:\", label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 578ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 109\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ ÙˆÛŒØ¯ÛŒÙˆ Ø²Ù†Ø¯Ù‡ ÛŒØ§ ÙˆÛŒØ¯ÛŒÙˆÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡\u001b[39;00m\n\u001b[0;32m    108\u001b[0m video_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppp.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙˆÛŒØ¯ÛŒÙˆØŒ Ø§ÛŒÙ† Ù…Ø³ÛŒØ± Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m \u001b[43mpredict_exercise_from_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_file\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙˆØ¨â€ŒÚ©Ù…ØŒ ÙˆÛŒØ¯ÛŒÙˆ Ø±Ø§ Ø®Ø§Ù„ÛŒ Ø¨Ú¯Ø°Ø§Ø±ÛŒØ¯ ÛŒØ§ None Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 60\u001b[0m, in \u001b[0;36mpredict_exercise_from_video\u001b[1;34m(video_path)\u001b[0m\n\u001b[0;32m     57\u001b[0m frame_resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, (\u001b[38;5;241m320\u001b[39m, \u001b[38;5;241m240\u001b[39m))\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m keypoints, landmarks \u001b[38;5;241m=\u001b[39m \u001b[43mextract_keypoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_resized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keypoints\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m NUM_FEATURES:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ Ù†Ø§Ø¯Ø±Ø³Øª Ø§Ø³Øª.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 31\u001b[0m, in \u001b[0;36mextract_keypoints\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03mØ§Ø² ÛŒÚ© ÙØ±ÛŒÙ…ØŒ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ (x,y) Ø¨Ø¯Ù† Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03mØ¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… Ø´Ù†Ø§Ø³Ø§ÛŒÛŒØŒ ÛŒÚ© Ø¢Ø±Ø§ÛŒÙ‡ Ø§Ø² ØµÙØ±Ù‡Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     30\u001b[0m frame_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 31\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:\n",
      "File \u001b[1;32mc:\\Users\\ML\\anaconda3\\envs\\xxxx\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ML\\anaconda3\\envs\\xxxx\\lib\\site-packages\\mediapipe\\python\\solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    361\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    362\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    363\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Ù…Ø±Ø­Ù„Ù‡ Ø´Ø´Ù…\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ scaler Ùˆ label encoder\n",
    "MODEL_PATH = \"lstm_model.h5\"\n",
    "SCALER_PATH = \"lstm_scaler.pkl\"\n",
    "LABEL_ENCODER_PATH = \"label_encoder.pkl\"\n",
    "\n",
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ØŒ scaler Ùˆ label encoder\n",
    "model = load_model(MODEL_PATH)\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "label_encoder = joblib.load(LABEL_ENCODER_PATH)\n",
    "\n",
    "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=1)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "# ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ: ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ 33 Ù†Ù‚Ø·Ù‡ Ø¨Ø§ 2 Ù…Ø®ØªØµØ§Øª (x,y) Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
    "NUM_FEATURES = 33 * 2\n",
    "\n",
    "def extract_keypoints(frame):\n",
    "    \"\"\"\n",
    "    Ø§Ø² ÛŒÚ© ÙØ±ÛŒÙ…ØŒ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ (x,y) Ø¨Ø¯Ù† Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
    "    Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… Ø´Ù†Ø§Ø³Ø§ÛŒÛŒØŒ ÛŒÚ© Ø¢Ø±Ø§ÛŒÙ‡ Ø§Ø² ØµÙØ±Ù‡Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.\n",
    "    \"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "    keypoints = []\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            keypoints.append(landmark.x)\n",
    "            keypoints.append(landmark.y)\n",
    "    else:\n",
    "        keypoints = [0.0] * NUM_FEATURES\n",
    "    return np.array(keypoints), results.pose_landmarks\n",
    "\n",
    "def predict_exercise_from_video(video_path=None):\n",
    "    cap = cv2.VideoCapture(video_path if video_path else 0)  # Ø§Ú¯Ø± ÙˆÛŒØ¯ÛŒÙˆ Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ø§Ø² ÙˆØ¨â€ŒÚ©Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†\n",
    "    if not cap.isOpened():\n",
    "        print(\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† ÙˆÛŒØ¯ÛŒÙˆ.\")\n",
    "        return None\n",
    "\n",
    "    # ØªÙ†Ø¸ÛŒÙ… Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙˆÛŒØ¯ÛŒÙˆ\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # ØªØºÛŒÛŒØ± Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ±ÛŒÙ… (Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²ØŒ Ù‡Ù…Ø§Ù† Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ØŒ Ù…Ø«Ù„Ø§Ù‹ 320x240)\n",
    "        frame_resized = cv2.resize(frame, (320, 240))\n",
    "\n",
    "        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ\n",
    "        keypoints, landmarks = extract_keypoints(frame_resized)\n",
    "        if keypoints.shape[0] != NUM_FEATURES:\n",
    "            print(\"âŒ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ Ù†Ø§Ø¯Ø±Ø³Øª Ø§Ø³Øª.\")\n",
    "            continue\n",
    "\n",
    "        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ scaler Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡\n",
    "        input_scaled = scaler.transform(keypoints.reshape(1, -1))\n",
    "\n",
    "        # ØªØºÛŒÛŒØ± Ø´Ú©Ù„ Ø¨Ù‡ Ø³Ù‡ Ø¨Ø¹Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ (1, NUM_FEATURES, 1)\n",
    "        input_final = input_scaled.reshape(1, NUM_FEATURES, 1)\n",
    "\n",
    "        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ù…Ø¯Ù„\n",
    "        prediction_probs = model.predict(input_final)\n",
    "        predicted_index = np.argmax(prediction_probs, axis=1)[0]\n",
    "        predicted_label = label_encoder.inverse_transform([predicted_index])[0]\n",
    "\n",
    "        # Ù†Ù…Ø§ÛŒØ´ Ù†ÙˆØ¹ Ø­Ø±Ú©Øª ÙˆØ±Ø²Ø´ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡\n",
    "        cv2.putText(frame, f\"Predicted: {predicted_label}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Ø±Ø³Ù… Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ Ùˆ Ø®Ø·ÙˆØ· Ø¨ÛŒÙ† Ù†Ù‚Ø§Ø·\n",
    "        if landmarks:\n",
    "            for i in range(33):  # Ø±Ø³Ù… Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ\n",
    "                x = int(landmarks.landmark[i].x * frame_width)\n",
    "                y = int(landmarks.landmark[i].y * frame_height)\n",
    "                cv2.circle(frame, (x, y), 5, (0, 255, 255), -1)  # Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ù‡ Ø±Ù†Ú¯ Ø²Ø±Ø¯\n",
    "\n",
    "            # Ø±Ø³Ù… Ø®Ø·ÙˆØ· Ø¨ÛŒÙ† Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ (ÙˆØµÙ„ Ú©Ø±Ø¯Ù† Ù†Ù‚Ø§Ø· Ù…Ø®ØªÙ„Ù Ø¨Ø¯Ù†)\n",
    "            connections = mp_pose.POSE_CONNECTIONS\n",
    "            for connection in connections:\n",
    "                start_idx, end_idx = connection\n",
    "                start = landmarks.landmark[start_idx]\n",
    "                end = landmarks.landmark[end_idx]\n",
    "                start_coords = int(start.x * frame_width), int(start.y * frame_height)\n",
    "                end_coords = int(end.x * frame_width), int(end.y * frame_height)\n",
    "                cv2.line(frame, start_coords, end_coords, (0, 255,0), 2)  # Ø®Ø·ÙˆØ· Ø¨Ù‡ Ø±Ù†Ú¯ Ù‚Ø±Ù…Ø²\n",
    "\n",
    "        # Ù†Ù…Ø§ÛŒØ´ ÙˆÛŒØ¯ÛŒÙˆ\n",
    "        cv2.imshow(\"Exercise Feedback\", frame)\n",
    "\n",
    "        # Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ú©Ø§Ø±Ø¨Ø± Ú©Ù„ÛŒØ¯ 'q' Ø±Ø§ Ø¨Ø²Ù†Ø¯ØŒ Ø§Ø² ÙˆÛŒØ¯ÛŒÙˆ Ø®Ø§Ø±Ø¬ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ ÙˆÛŒØ¯ÛŒÙˆ Ø²Ù†Ø¯Ù‡ ÛŒØ§ ÙˆÛŒØ¯ÛŒÙˆÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡\n",
    "video_file = \"ppp.mp4\"  # Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙˆÛŒØ¯ÛŒÙˆØŒ Ø§ÛŒÙ† Ù…Ø³ÛŒØ± Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯\n",
    "predict_exercise_from_video(video_file)  # Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙˆØ¨â€ŒÚ©Ù…ØŒ ÙˆÛŒØ¯ÛŒÙˆ Ø±Ø§ Ø®Ø§Ù„ÛŒ Ø¨Ú¯Ø°Ø§Ø±ÛŒØ¯ ÛŒØ§ None Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xxxx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
